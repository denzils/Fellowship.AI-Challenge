{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fellowship.AI Challenge\n",
    "### Denzil Sikka\n",
    "\n",
    "I'm trying to build a basic recommender system based on the Lab41 MovieLens dataset. I used the MovieLens 100K Dataset from 1000 users on 1700 movies. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import cross_validation as cv\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the above libraries to build this recommendation engine.\n",
    "\n",
    "I then read in the data file u.data. From the full u data set, 100000 ratings by 943 users on 1682 items. Each user has rated at least 20 movies.  Users and items are numbered consecutively from 1.  The data is randomly ordered. This is a tab separated list of \n",
    "user id | item id | rating | timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 943 users and 1682 movies in this dataset.\n"
     ]
    }
   ],
   "source": [
    "r_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\n",
    "ratings = pd.read_csv('u.data', sep='\\t', names=r_cols,\n",
    " encoding='latin-1')\n",
    "\n",
    "\n",
    "num_users = ratings.user_id.unique().shape[0]\n",
    "num_items = ratings.movie_id.unique().shape[0]\n",
    "print('There are ' + str(num_users) + ' users and ' + str(num_items) + ' movies in this dataset.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the scikit-learn library to split the dataset into testing and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Ratings Data - 75%\n",
      "(75000, 4)\n",
      "       user_id  movie_id  rating  unix_timestamp\n",
      "35787       23       230       4       874785809\n",
      "47269      505       202       3       889333508\n",
      "63077        9       201       5       886960055\n",
      "41787      648       118       4       882212200\n",
      "97897      847       133       3       878941027\n",
      "\n",
      "Testing Ratings Data - 25%\n",
      "(25000, 4)\n",
      "       user_id  movie_id  rating  unix_timestamp\n",
      "10295        9       340       4       886958715\n",
      "9969       262       195       2       879791755\n",
      "98526      870      1014       2       884789665\n",
      "83713      453       202       4       877553999\n",
      "49993      407         8       5       875042425\n"
     ]
    }
   ],
   "source": [
    "training_ratings_data, testing_ratings_data = cv.train_test_split(ratings, test_size=0.25)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training Ratings Data - 75%\")\n",
    "print(training_ratings_data.shape)\n",
    "print(training_ratings_data.head())\n",
    "print(\"\")\n",
    "print(\"Testing Ratings Data - 25%\")\n",
    "print(testing_ratings_data.shape)\n",
    "print(testing_ratings_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then created user-item matrix and then calculated two types of similarity: Item-Item and User-Item. Item-Item Collaborative Filterning is measured by observing users who have rated both the same items. User-Item Collaborative Filtering is measured between users by observing all the items rated by both users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User-Item Matrix\n",
      "\n",
      "[[ 0.  3.  0. ...,  0.  0.  0.]\n",
      " [ 4.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 5.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  5.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "training_ratings_matrix = np.zeros((num_users, num_items))\n",
    "\n",
    "for row in training_ratings_data.itertuples():\n",
    "    training_ratings_matrix[row[1]-1, row[2]-1] = row[3]\n",
    "\n",
    "testing_ratings_matrix = np.zeros((num_users, num_items))\n",
    "\n",
    "for row in testing_ratings_data.itertuples():\n",
    "    testing_ratings_matrix[row[1]-1, row[2]-1] = row[3]\n",
    "\n",
    "print(\"\")\n",
    "print(\"User-Item Matrix\")\n",
    "print(\"\")\n",
    "print(training_ratings_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several different types of similarity metrics in recommender systems. In this case, I used cosine similarity where the ratings are seen as vectors in n-dimensional space and the similarity is based on the cosine of the angle between these vectors. The smaller the angle between the two vectors then larger the cosine value.\n",
    "\n",
    "To calculate similarity between two vectors, you take their dot product and then divide by the product of the Euclidean lengths of the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User Similarity\n",
      "[[ -1.99840144e-15   8.89420763e-01   9.74215339e-01 ...,   9.41423031e-01\n",
      "    8.65712798e-01   7.57741299e-01]\n",
      " [  8.89420763e-01   1.11022302e-16   9.11208786e-01 ...,   9.15725017e-01\n",
      "    9.00824165e-01   9.75989936e-01]\n",
      " [  9.74215339e-01   9.11208786e-01   0.00000000e+00 ...,   9.55166416e-01\n",
      "    8.90081850e-01   9.84672217e-01]\n",
      " ..., \n",
      " [  9.41423031e-01   9.15725017e-01   9.55166416e-01 ...,   1.11022302e-16\n",
      "    8.94927475e-01   9.54415769e-01]\n",
      " [  8.65712798e-01   9.00824165e-01   8.90081850e-01 ...,   8.94927475e-01\n",
      "    1.11022302e-16   8.55512219e-01]\n",
      " [  7.57741299e-01   9.75989936e-01   9.84672217e-01 ...,   9.54415769e-01\n",
      "    8.55512219e-01   0.00000000e+00]]\n",
      "\n",
      "Item Similarity\n",
      "\n",
      "[[  9.99200722e-16   7.23215593e-01   7.60700484e-01 ...,   1.00000000e+00\n",
      "    1.00000000e+00   9.46859981e-01]\n",
      " [  7.23215593e-01   2.22044605e-16   8.32015536e-01 ...,   1.00000000e+00\n",
      "    1.00000000e+00   9.06432571e-01]\n",
      " [  7.60700484e-01   8.32015536e-01   1.11022302e-16 ...,   1.00000000e+00\n",
      "    1.00000000e+00   8.92993544e-01]\n",
      " ..., \n",
      " [  1.00000000e+00   1.00000000e+00   1.00000000e+00 ...,   0.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00]\n",
      " [  1.00000000e+00   1.00000000e+00   1.00000000e+00 ...,   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00]\n",
      " [  9.46859981e-01   9.06432571e-01   8.92993544e-01 ...,   1.00000000e+00\n",
      "    1.00000000e+00   0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "user_similarity = pairwise_distances(training_ratings_matrix, metric='cosine')\n",
    "item_similarity = pairwise_distances(training_ratings_matrix.T, metric='cosine')\n",
    "print(\"\")\n",
    "print(\"User Similarity\")\n",
    "print(user_similarity)\n",
    "print(\"\")\n",
    "print(\"Item Similarity\")\n",
    "print(\"\")\n",
    "print(item_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the similarity betwen two users as a weight that is multiplied by the rating of one of the users as a means of predicting that user's rating. I did correct for how each user rates items on average because not every user rates in the same way. This is a very rough method in order to correct for that subjectiveness. This also only mattered in the User-User similarity prediction scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Item Prediction\n",
      "[[ 0.34135254  0.35083897  0.37325794 ...,  0.41121571  0.40844233\n",
      "   0.39707148]\n",
      " [ 0.08759883  0.10248107  0.09951639 ...,  0.10285924  0.10463734\n",
      "   0.10510163]\n",
      " [ 0.07074828  0.07400544  0.07215868 ...,  0.06849997  0.0725327\n",
      "   0.0735303 ]\n",
      " ..., \n",
      " [ 0.03241162  0.04129133  0.03947997 ...,  0.04523056  0.0451843\n",
      "   0.04525505]\n",
      " [ 0.1135916   0.12297656  0.13049496 ...,  0.13698888  0.1373365\n",
      "   0.13692521]\n",
      " [ 0.19475647  0.19363828  0.21354984 ...,  0.24458693  0.24256837\n",
      "   0.2362278 ]]\n",
      "\n",
      "Mean User Rating\n",
      "[ 0.40844233  0.10463734  0.0725327   0.0332937   0.22235434  0.34066587\n",
      "  0.72770511  0.09036861  0.04221165  0.35909631]\n",
      "\n",
      "User Prediction\n",
      "[[ 1.63894444  0.52528916  0.46802603 ...,  0.25810904  0.25562256\n",
      "   0.25785514]\n",
      " [ 1.39544438  0.27495301  0.17152398 ..., -0.06434232 -0.06624518\n",
      "  -0.06276104]\n",
      " [ 1.41344689  0.23519124  0.14342942 ..., -0.09601814 -0.09740923\n",
      "  -0.09397189]\n",
      " ..., \n",
      " [ 1.26853077  0.20031025  0.102637   ..., -0.11937305 -0.12157668\n",
      "  -0.11828463]\n",
      " [ 1.41171497  0.28085977  0.20881624 ..., -0.02176678 -0.02392171\n",
      "  -0.02077531]\n",
      " [ 1.46720113  0.35240544  0.30034764 ...,  0.09209355  0.08962797\n",
      "   0.09207687]]\n"
     ]
    }
   ],
   "source": [
    "item_prediction = training_ratings_matrix.dot(item_similarity) / np.array([np.abs(item_similarity).sum(axis=1)])\n",
    "print(\"\")\n",
    "print(\"Item Prediction\")\n",
    "print(item_prediction)\n",
    "print(\"\")\n",
    "mean_user_rating = training_ratings_matrix.mean(axis=1)\n",
    "print(\"Mean User Rating\")\n",
    "print(mean_user_rating[0:10])\n",
    "ratings_diff = (training_ratings_matrix - mean_user_rating[:, np.newaxis])\n",
    "user_prediction = mean_user_rating[:, np.newaxis] + user_similarity.dot(ratings_diff) / np.array([np.abs(user_similarity).sum(axis=1)]).T\n",
    "print(\"\")\n",
    "print(\"User Prediction\")\n",
    "print(user_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I only wanted to consider predicted ratings that are in the test dataset so I filtered out all other elements. I scaled user ratings to be back out of 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User Ratings Prediction for Test Data Set\n",
      "[5, 2.0, 5, 4.0, 2.0, 4.0, 3.0, 5.0, 2.0, 1.0]\n",
      "\n",
      "User Test Data Set\n",
      "[ 5.  4.  4.  5.  3.  4.  3.  4.  5.  2.]\n",
      "\n",
      "\n",
      "Item Ratings Prediction for Test Data Set\n",
      "[2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0]\n",
      "\n",
      "Item Test Data Set\n",
      "[ 5.  4.  4.  5.  3.  4.  3.  4.  5.  2.]\n"
     ]
    }
   ],
   "source": [
    "user_ratings_prediction = user_prediction[testing_ratings_matrix.nonzero()].flatten()\n",
    "ratings_five = [min(round(i*5), 5) for i in user_ratings_prediction]\n",
    "user_ratings_prediction = ratings_five\n",
    "user_testing_ratings_prediction = testing_ratings_matrix[testing_ratings_matrix.nonzero()].flatten()\n",
    "print(\"\")\n",
    "print(\"User Ratings Prediction for Test Data Set\")\n",
    "print(user_ratings_prediction[0:10])\n",
    "print(\"\")\n",
    "print(\"User Test Data Set\")\n",
    "print(user_testing_ratings_prediction[0:10])\n",
    "print(\"\")\n",
    "\n",
    "item_ratings_prediction = item_prediction[testing_ratings_matrix.nonzero()].flatten()\n",
    "ratings_five = [min(round(i*5), 5) for i in item_ratings_prediction]\n",
    "item_ratings_prediction = ratings_five\n",
    "item_testing_ratings_prediction = testing_ratings_matrix[testing_ratings_matrix.nonzero()].flatten()\n",
    "print(\"\")\n",
    "print(\"Item Ratings Prediction for Test Data Set\")\n",
    "print(item_ratings_prediction[0:10])\n",
    "print(\"\")\n",
    "print(\"Item Test Data Set\")\n",
    "print(item_testing_ratings_prediction[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the popular metric Root Mean Squared Error (RMSE) to evaluate the accuracy of the predicted ratings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-based CF RMSE\n",
      "1.8255300600099686\n",
      "\n",
      "User-based CF RMSE\n",
      "2.654648752660133\n"
     ]
    }
   ],
   "source": [
    "user_prediction_error_eval = sqrt(mean_squared_error(user_ratings_prediction, user_testing_ratings_prediction))\n",
    "print('User-based CF RMSE')\n",
    "print(user_prediction_error_eval)\n",
    "print(\"\")\n",
    "item_prediction_error_eval = sqrt(mean_squared_error(item_ratings_prediction, item_testing_ratings_prediction))\n",
    "print('User-based CF RMSE')\n",
    "print(item_prediction_error_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This memory-based algorithm is easy to implement and produced reasonable predictions; however, it does not address the cold-start problem (when a new user or new item enters the system and there is nothing to judge by) and it is difficult to scale. It is not the best method for the sparsity level of the MovieLens dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sparsity level of MovieLens100K is 93.7%\n"
     ]
    }
   ],
   "source": [
    "sparsity=round(1.0-len(ratings)/float(num_users*num_items),3)\n",
    "print('The sparsity level of MovieLens100K is ' +  str(sparsity*100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model-based collaborative filtering method is scalable and can deal with higher sparsity levels, compared to this memory-based mode. It still suffers from the cold-start problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
